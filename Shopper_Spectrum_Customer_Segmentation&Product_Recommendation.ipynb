{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vncDsAP0Gaoa"
      },
      "source": [
        "# **Project Name**    -\n",
        "##Shopper Spectrum: Customer Segmentation and Product Recommendations in E-Commerce\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beRrZCGUAJYm"
      },
      "source": [
        "##### **Project Type**    - EDA/Unsupervised/Clustering   &  CollaborativeFiltering/Recommendation System                           \n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member**     - Dhruv Tamirisa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJNUwmbgGyua"
      },
      "source": [
        "# **Project Summary -**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6v_1wHtG2nS"
      },
      "source": [
        "This project, \"Shopper Spectrum,\" focuses on analyzing e-commerce transaction data to understand customer behavior. The primary goals are to segment customers using the below:\n",
        "\n",
        "RFM (Recency, Frequency, Monetary) analysis and K-Means clustering, and to build an item-based collaborative filtering model for product recommendations. The project involves extensive data preprocessing, exploratory data analysis (EDA), and feature engineering. The final output is  interactive\n",
        "\n",
        "Streamlit web application where users can get product recommendations and predict customer segments based on real-time inputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6K7xa23Elo4"
      },
      "source": [
        "# **GitHub Link -**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1o69JH3Eqqn"
      },
      "source": [
        "https://github.com/DhruvTamirisa/Shopper-Spectrum-Customer-Segmentation-Product-Recommendation-Labmentix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQaldy8SH6Dl"
      },
      "source": [
        "# **Problem Statement**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpeJGUA3kjGy"
      },
      "source": [
        "The global e-commerce industry generates vast amounts of transaction data daily, offering valuable insights into customer purchasing behaviors. Analyzing this data is essential for identifying meaningful customer segments and recommending relevant products to enhance customer experience and drive business growth. This project aims to examine transaction data from an online retail business to uncover patterns in customer purchase behavior, segment customers based on Recency, Frequency, and Monetary (RFM) analysis, and develop a product recommendation system using collaborative filtering techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDgbUHAGgjLW"
      },
      "source": [
        "# **General Guidelines** : -  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      },
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_i_v8NEhb9l"
      },
      "source": [
        "# ***Let's Begin !***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhfV-JJviCcP"
      },
      "source": [
        "## ***1. Know Your Data***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3lxredqlCYt"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RnN4peoiCZX"
      },
      "source": [
        "### Dataset Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv('online_retail.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x71ZqKXriCWQ"
      },
      "source": [
        "### Dataset First View"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hvnu-JrC54IT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hBIi_osiCS2"
      },
      "source": [
        "### Dataset Rows & Columns count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlHwYmJAmNHm"
      },
      "source": [
        "### Dataset Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35m5QtbWiB9F"
      },
      "source": [
        "#### Duplicate Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "outputs": [],
      "source": [
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoPl-ycgm1ru"
      },
      "source": [
        "#### Missing Values/Null Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(df.isnull(), cbar=False, yticklabels=False)\n",
        "plt.title('Missing Value Heatmap')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0kj-8xxnORC"
      },
      "source": [
        "### What did you know about your dataset?\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfoNAAC-nUe_"
      },
      "source": [
        "This dataset contains transnational e-commerce data from 2022-2023, with columns like\n",
        "**InvoiceNo, StockCode, Quantity, UnitPrice,** and **CustomerID** that describe customer transactions. Initial analysis shows that it contains a significant number of duplicate entries and missing values, particularly in the\n",
        "**CustomerID** column. These issues must be addressed in the data preprocessing step to prepare the data for analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      },
      "source": [
        "## ***2. Understanding Your Variables***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBTbrJXOngz2"
      },
      "source": [
        "### Variables Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJV4KIxSnxay"
      },
      "source": [
        "**Transaction & Product ID's:*** The dataset includes **InvoiceNo** for the transaction number, **StockCode** for the unique product code, and **Description** for the product's name.\n",
        "\n",
        "\n",
        "**Purchase Details:** **Quantity** represents the number of products bought, while **UnitPrice** indicates the price for each product.\n",
        "\n",
        "\n",
        "**Customer & Location Information:** **CustomerID** is a unique identifier for each shopper, **Country** is their location, and **InvoiceDate** is the date and time of the transaction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3PMJOP6ngxN"
      },
      "source": [
        "### Check Unique Values for each variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "outputs": [],
      "source": [
        "df.nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dauF4eBmngu3"
      },
      "source": [
        "## 3. ***Data Wrangling***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKJF3rekwFvQ"
      },
      "source": [
        "### Data Wrangling Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "print(f\"Initial dataset shape: {df.shape}\")\n",
        "# Handle missing CustomerIDs as per the project requirements\n",
        "df.dropna(subset=['CustomerID'], inplace=True)\n",
        "# Remove duplicate rows\n",
        "df.drop_duplicates(inplace=True)\n",
        "# Exclude cancelled invoices and invalid transactions\n",
        "df = df[~df['InvoiceNo'].astype(str).str.startswith('C')]\n",
        "df = df[df['Quantity'] > 0]\n",
        "df = df[df['UnitPrice'] > 0]\n",
        "print(f\"Shape after cleaning and removing duplicates: {df.shape}\")\n",
        "# Convert data types and create TotalPrice\n",
        "df['CustomerID'] = df['CustomerID'].astype(int).astype(str)\n",
        "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
        "df['TotalPrice'] = df['Quantity'] * df['UnitPrice']\n",
        "# Calculate RFM values\n",
        "snapshot_date = df['InvoiceDate'].max() + pd.Timedelta(days=1)\n",
        "rfm_df = df.groupby('CustomerID').agg(\n",
        "    Recency=('InvoiceDate', lambda date: (snapshot_date - date.max()).days),\n",
        "    Frequency=('InvoiceNo', 'nunique'),\n",
        "    Monetary=('TotalPrice', 'sum')\n",
        ")\n",
        "print(\"\\nRFM DataFrame created:\")\n",
        "print(rfm_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "MvQLRl_1-5BT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSa1f5Uengrz"
      },
      "source": [
        "### What all manipulations have you done and insights you found?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbyXE7I1olp8"
      },
      "source": [
        "The data was cleaned by removing rows with missing\n",
        "CustomerIDs, canceling invalid transactions, and correcting data types. I then engineered\n",
        "Recency, Frequency, and Monetary (RFM) features to pivot the data from a transactional log to a customer-centric summary. The main insight is that the raw data is quite noisy and must be significantly refined to be useful for customer segmentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF8Ens_Soomf"
      },
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wOQAZs5pc--"
      },
      "source": [
        "#### Chart 1: Top 10 Countries by Transaction Volume"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "outputs": [],
      "source": [
        "sns.set(style=\"whitegrid\")\n",
        "# Create the plot\n",
        "plt.figure(figsize=(12, 8))\n",
        "top_countries = df['Country'].value_counts().nlargest(10)\n",
        "sns.barplot(x=top_countries.values, y=top_countries.index, palette='viridis')\n",
        "# Add labels and title\n",
        "plt.title('Top 10 Countries by Transaction Volume', fontsize=16)\n",
        "plt.xlabel('Number of Transactions', fontsize=12)\n",
        "plt.ylabel('Country', fontsize=12)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5QZ13OEpz2H"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XESiWehPqBRc"
      },
      "source": [
        "A horizontal bar chart is ideal for comparing the number of transactions across different categories (countries). It's easy to read and effectively highlights the key markets, which is a primary goal of the EDA phase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_j1G7yiqdRP"
      },
      "source": [
        "This visualization will likely show that the vast majority of transactions originate from the United Kingdom, indicating that it is the business's core market. The other countries will have significantly fewer transactions in comparison."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "448CDAPjqfQr"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cspy4FjqxJW"
      },
      "source": [
        "Yes. Knowing your key markets helps focus marketing efforts and budget. It can also inform decisions about logistics, shipping policies, and language localization for the website, directly impacting customer experience and sales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSlN3yHqYklG"
      },
      "source": [
        "#### Chart 2: Top 10 Best-Selling Products"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "top_products = df['Description'].value_counts().nlargest(10)\n",
        "sns.barplot(x=top_products.values, y=top_products.index, palette='magma')\n",
        "plt.title('Top 10 Best-Selling Products', fontsize=16)\n",
        "plt.xlabel('Number of Times Purchased', fontsize=12)\n",
        "plt.ylabel('Product Description', fontsize=12)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6dVpIINYklI"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aaW0BYyYklI"
      },
      "source": [
        "A horizontal bar chart is again used because it clearly displays and ranks the top products, even if their names are long. This directly addresses the task of identifying top-selling items."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijmpgYnKYklI"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSx9atu2YklI"
      },
      "source": [
        "The chart reveals the \"hero\" products that are most frequently purchased by customers. These are the items that consistently drive sales volume."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JiQyfWJYklI"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcBbebzrYklV"
      },
      "source": [
        "Absolutely. This information is vital for\n",
        "inventory management to avoid stockouts of popular items.  These products can also be used in marketing campaigns, featured on the homepage, or bundled with other items to increase overall sales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EM7whBJCYoAo"
      },
      "source": [
        "#### Chart 3: RFM Value Distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "outputs": [],
      "source": [
        "# Create figure and axes for subplots\n",
        "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
        "# Plot Recency distribution\n",
        "sns.histplot(rfm_df['Recency'], bins=30, ax=axes[0], kde=True, color='skyblue')\n",
        "axes[0].set_title('Recency Distribution', fontsize=14)\n",
        "# Plot Frequency distribution\n",
        "sns.histplot(rfm_df['Frequency'], bins=30, ax=axes[1], kde=True, color='olive')\n",
        "axes[1].set_title('Frequency Distribution', fontsize=14)\n",
        "# Plot Monetary distribution\n",
        "sns.histplot(rfm_df['Monetary'], bins=30, ax=axes[2], kde=True, color='gold')\n",
        "axes[2].set_title('Monetary Distribution', fontsize=14)\n",
        "# Show the plots\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fge-S5ZAYoAp"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dBItgRVYoAp"
      },
      "source": [
        "Histograms (or distribution plots) are the best way to visualize the distribution of single numerical variables. They show the underlying frequency distribution, central tendency, and skewness of the RFM values, which is crucial for understanding the data before applying a clustering algorithm like K-Means."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85gYPyotYoAp"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jstXR6OYoAp"
      },
      "source": [
        "You will likely find that Frequency and Monetary are highly right-skewed, meaning most customers purchase infrequently and spend small amounts, while a few customers are responsible for a large number of purchases and revenue. Recency might be more evenly distributed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoGjAbkUYoAp"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      },
      "source": [
        "Yes. Understanding these distributions is fundamental to customer segmentation. The heavy skew in Frequency and Monetary suggests that a \"one-size-fits-all\" marketing approach is ineffective. It also indicates that data transformation techniques (like log transformation) may be necessary to improve the performance of the K-Means clustering model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Of9eVA-YrdM"
      },
      "source": [
        "#### Chart 4: Sales Trends Over Time (Monthly)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "outputs": [],
      "source": [
        "# Resample data to get total sales per month\n",
        "monthly_sales = df.set_index('InvoiceDate')['TotalPrice'].resample('M').sum()\n",
        "plt.figure(figsize=(15, 7))\n",
        "monthly_sales.plot(kind='line', marker='o', color='teal')\n",
        "plt.title('Total Sales Over Time (Monthly)', fontsize=16)\n",
        "plt.xlabel('Month', fontsize=12)\n",
        "plt.ylabel('Total Sales', fontsize=12)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iky9q4vBYrdO"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJRCwT6DYrdO"
      },
      "source": [
        "A line chart is the standard and most effective way to display a time-series variable like sales data. It clearly shows trends, peaks, and troughs over a continuous period, which directly addresses the project task of visualizing purchase trends over time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6T5p64dYrdO"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      },
      "source": [
        "This chart will likely reveal strong seasonality in the business, with a significant spike in sales during the holiday season (October to December). Other months may show stable or slightly fluctuating sales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-Ehk30pYrdP"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLNxxz7MYrdP"
      },
      "source": [
        "Yes. Identifying seasonality is crucial for\n",
        "inventory management and stock optimization. The business can prepare for peak demand by increasing stock and can run targeted marketing campaigns before and during these high-sales periods to maximize revenue.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bamQiAODYuh1"
      },
      "source": [
        "####Chart 5: Distribution of Monetary Value per Transaction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "outputs": [],
      "source": [
        "# Group by InvoiceNo to get the total monetary value of each transaction\n",
        "invoice_monetary = df.groupby('InvoiceNo')['TotalPrice'].sum()\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.histplot(invoice_monetary, bins=1000, kde=False, color='indigo')\n",
        "plt.title('Distribution of Monetary Value per Transaction', fontsize=16)\n",
        "plt.xlabel('Monetary Value', fontsize=12)\n",
        "plt.ylabel('Number of Transactions', fontsize=12)\n",
        "plt.xlim(0, 1000) # Limiting to see the bulk of transactions\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcxuIMRPYuh3"
      },
      "source": [
        "A histogram is perfect for understanding the distribution of a single numerical variable like transaction value. It clearly shows that the vast majority of transactions are of lower value, which might not be as obvious with a simple summary statistic like the mean"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwzvFGzlYuh3"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyqkiB8YYuh3"
      },
      "source": [
        "The chart will demonstrate that the transaction values are heavily right-skewed. Most purchases are of a relatively low value, while a long tail of high-value transactions exists but is infrequent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYpmQ266Yuh3"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      },
      "source": [
        "This insight is valuable for\n",
        "dynamic pricing strategies and promotions. For example, the business could implement strategies like offering free shipping on orders above a certain threshold (e.g., just above the peak of the distribution) to encourage customers to increase their average order value.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH-pJp9IphqM"
      },
      "source": [
        "#### Chart 6: Correlation Heatmap of RFM Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "outputs": [],
      "source": [
        "#correlation matrix for RFM\n",
        "rfm_corr = rfm_df[['Recency', 'Frequency', 'Monetary']].corr()\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(rfm_corr, annot=True, fmt='.2f', cmap='coolwarm', vmin=-1, vmax=1)\n",
        "plt.title('Correlation Heatmap of RFM Features', fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbFf2-_FphqN"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loh7H2nzphqN"
      },
      "source": [
        "A heatmap is the best way to visualize a correlation matrix. The colors and annotations provide an immediate understanding of the strength and direction of the relationships between the RFM variables, which is a key part of the EDA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ouA3fa0phqN"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VECbqPI7phqN"
      },
      "source": [
        "It will likely show a strong positive correlation between Frequency and Monetary, indicating that customers who purchase often tend to spend more overall. It may also show a negative correlation between Recency and the other two metrics, suggesting that customers who haven't purchased recently are less frequent and have spent less."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Seke61FWphqN"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DW4_bGpfphqN"
      },
      "source": [
        "Yes. This confirms the validity of the RFM model for this dataset. The strong F-M correlation helps in labeling clusters; for example, a cluster with high Frequency and high Monetary values can be confidently labeled as\n",
        "\"High-Value\" customers. This allows for more effective targeted marketing and retention programs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIIx-8_IphqN"
      },
      "source": [
        "#### Chart 7: Pairplot of RFM Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "outputs": [],
      "source": [
        "# Create a pairplot to visualize relationships between RFM variables\n",
        "sns.pairplot(rfm_df, diag_kind='kde',\n",
        "             plot_kws={'alpha': 0.6, 's': 80, 'edgecolor': 'k'},\n",
        "             height=4)\n",
        "plt.suptitle('Pairwise Relationships of RFM Features', size=20, y=1.02)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t27r6nlMphqO"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iv6ro40sphqO"
      },
      "source": [
        "A pairplot is an excellent tool for exploring the relationships between multiple numerical variables at once. It combines scatter plots (to show correlations) and kernel density plots (to show distributions), providing a comprehensive overview that is perfect for understanding the landscape of the customer data before segmentation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2jJGEOYphqO"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po6ZPi4hphqO"
      },
      "source": [
        "The scatter plots will visually confirm the correlations seen in the heatmap (e.g., the positive relationship between Frequency and Monetary). More importantly, they can reveal the shape of the data and potential clusters that might not be purely linear."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0JNsNcRphqO"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvSq8iUTphqO"
      },
      "source": [
        "Yes. This visualization helps in qualitatively assessing the data's suitability for clustering. By understanding the relationships visually, you can better interpret the segments that the K-Means algorithm will eventually identify, leading to more meaningful customer personas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZR9WyysphqO"
      },
      "source": [
        "#### Chart - 8: Elbow Curve for K-Means Cluster Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "rfm_scaled = scaler.fit_transform(rfm_df)\n",
        "inertia = []\n",
        "k_range = range(1, 11)\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(rfm_scaled)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "# Create the Elbow Curve plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(k_range, inertia, marker='o', linestyle='--')\n",
        "plt.title('Elbow Curve for K-Means Clustering', fontsize=16)\n",
        "plt.xlabel('Number of Clusters (k)', fontsize=12)\n",
        "plt.ylabel('Inertia', fontsize=12)\n",
        "plt.xticks(k_range)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jj7wYXLtphqO"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ob8u6rCTphqO"
      },
      "source": [
        "The Elbow Curve is the standard and most intuitive method for selecting the optimal value of 'k' (the number of clusters) for the K-Means algorithm. It's an essential step for building a methodologically sound clustering model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZrbJ2SmphqO"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZtgC_hjphqO"
      },
      "source": [
        "The chart plots the model's inertia (a measure of how internally coherent the clusters are) against different values of 'k'. You will see the inertia decrease as 'k' increases. The \"elbow point\"—where the rate of decrease sharply slows down—indicates the most appropriate number of clusters to use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFu4xreNphqO"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ey_0qi68phqO"
      },
      "source": [
        "Absolutely. Choosing the correct number of segments is fundamental to the project's success. Too few clusters will group dissimilar customers together, while too many will make the segments too small to be meaningful. This chart ensures your segmentation strategy is based on the actual structure of the data, leading to more effective and targeted marketing campaigns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJ55k-q6phqO"
      },
      "source": [
        "#### Chart 9: Customer Cluster Profiles (3D Scatter Plot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "#This code assumes you have chosen the optimal k (e.g., 4)\n",
        "# run the K-Means algorithm to get cluster labels      ---\n",
        "\n",
        "# Scale RFM data\n",
        "scaler = StandardScaler()\n",
        "rfm_scaled = scaler.fit_transform(rfm_df[['Recency', 'Frequency', 'Monetary']])\n",
        "\n",
        "\n",
        "optimal_k = 4\n",
        "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
        "rfm_df['Cluster'] = kmeans.fit_predict(rfm_scaled)\n",
        "fig = px.scatter_3d(rfm_df,\n",
        "                    x='Recency',\n",
        "                    y='Frequency',\n",
        "                    z='Monetary',\n",
        "                    color='Cluster',\n",
        "                    symbol='Cluster',\n",
        "                    size_max=18,\n",
        "                    opacity=0.7,\n",
        "                    title='3D Visualization of Customer Segments')\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCFgpxoyphqP"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVxDimi2phqP"
      },
      "source": [
        "A 3D scatter plot is the most powerful way to visualize the customer segments across all three RFM dimensions simultaneously. It provides an intuitive and clear picture of how the clusters are separated in the feature space, which directly addresses the \"Customer cluster profiles\" task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVtJsKN_phqQ"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngGi97qjphqQ"
      },
      "source": [
        "The plot will show distinct clouds of data points, with each color representing a different customer segment. You will be able to visually identify your key segments, such as \"High-Value\" customers (low Recency, high Frequency & Monetary) or \"At-Risk\" customers (high Recency, low Frequency & Monetary)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lssrdh5qphqQ"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBpY5ekJphqQ"
      },
      "source": [
        "This visualization is extremely valuable for communicating the results to business stakeholders. It makes the abstract concept of \"customer segments\" tangible, allowing marketing and sales teams to better understand the different customer personas and develop tailored strategies to engage each one effectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      },
      "source": [
        "#### Chart 10: Cluster Profiles by Average RFM Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "outputs": [],
      "source": [
        "# This code assumes you have the rfm_df with the 'Cluster' column from Chart 9\n",
        "# Calculate the average RFM values for each cluster\n",
        "cluster_summary = rfm_df.groupby('Cluster').agg({\n",
        "    'Recency': 'mean',\n",
        "    'Frequency': 'mean',\n",
        "    'Monetary': 'mean'\n",
        "}).round(2)\n",
        "print(\"Average RFM Values per Cluster:\")\n",
        "print(cluster_summary)\n",
        "# Create subplots\n",
        "fig, axes = plt.subplots(1, 3, figsize=(20, 7))\n",
        "fig.suptitle('Average RFM Values for Each Customer Segment', fontsize=18)\n",
        "# Plot Average Recency\n",
        "sns.barplot(x=cluster_summary.index, y=cluster_summary['Recency'], ax=axes[0], palette='coolwarm')\n",
        "axes[0].set_title('Average Recency')\n",
        "# Plot Average Frequency\n",
        "sns.barplot(x=cluster_summary.index, y=cluster_summary['Frequency'], ax=axes[1], palette='coolwarm')\n",
        "axes[1].set_title('Average Frequency')\n",
        "# Plot Average Monetary\n",
        "sns.barplot(x=cluster_summary.index, y=cluster_summary['Monetary'], ax=axes[2], palette='coolwarm')\n",
        "axes[2].set_title('Average Monetary Value')\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1M8mcRywphqQ"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8agQvks0phqQ"
      },
      "source": [
        "Bar charts are perfect for comparing the average values of a metric across different categories (the clusters). This provides a straightforward, quantitative view of each segment's defining characteristics, which is essential for labeling them (e.g., High-Value, At-Risk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgIPom80phqQ"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qp13pnNzphqQ"
      },
      "source": [
        "The charts will clearly show the profile of each segment. For example, one cluster will have low average Recency but high Frequency and Monetary averages, identifying them as your \"Best Customers.\" Another will have high Recency and low Frequency/Monetary, identifying them as \"At-Risk.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMzcOPDDphqR"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4Ka1PC2phqR"
      },
      "source": [
        "Yes. This is one of the most impactful visualizations. It allows the business to translate the machine learning model's output into actionable personas. Marketing can then design highly specific campaigns, such as loyalty rewards for the best customers and re-engagement offers for the at-risk group"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-EpHcCOp1ci"
      },
      "source": [
        "#### Chart 11: Customer Segment Size and Proportion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "outputs": [],
      "source": [
        "# Calculate the number of customers in each cluster\n",
        "segment_counts = rfm_df['Cluster'].value_counts()\n",
        "# Create the pie chart\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.pie(segment_counts, labels=segment_counts.index, autopct='%1.1f%%', startangle=140,\n",
        "        wedgeprops={'edgecolor': 'black'}, textprops={'fontsize': 12})\n",
        "\n",
        "plt.title('Proportion of Customers in Each Segment', fontsize=16)\n",
        "plt.axis('equal') # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_VqEhTip1ck"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vsMzt_np1ck"
      },
      "source": [
        "A pie chart is an intuitive way to show the proportional distribution of a whole. It instantly communicates what percentage of the total customer base belongs to each segment, which is a key piece of information for strategic planning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zGJKyg5p1ck"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      },
      "source": [
        "The chart reveals the relative size of each customer segment. For instance, you might find that your most valuable \"High-Value\" customers make up only a small percentage of your total customer base, while a large percentage might be \"Occasional\" shoppers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "druuKYZpp1ck"
      },
      "source": [
        "Definitely. This insight helps the business allocate resources effectively. It might justify spending more on personalized retention for the small but crucial high-value segment, or launching a large-scale campaign to nurture the large occasional shopper segment into more regular buyers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3dbpmDWp1ck"
      },
      "source": [
        "####Chart 12: Product Recommendation Similarity Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "# To make the matrix readable, we'll visualize for top N selling products\n",
        "# Get top 20 selling products\n",
        "top_products_list = df['Description'].value_counts().nlargest(20).index.tolist()\n",
        "# Filter the dataframe to only include these top products\n",
        "top_products_df = df[df['Description'].isin(top_products_list)]\n",
        "\n",
        "# Create the customer-item utility matrix (1 if bought, 0 if not)\n",
        "utility_matrix = top_products_df.pivot_table(index='CustomerID', columns='Description', values='Quantity', aggfunc='count', fill_value=0)\n",
        "utility_matrix = utility_matrix.applymap(lambda x: 1 if x > 0 else 0)\n",
        "# Compute cosine similarity between items (products)\n",
        "product_similarity_matrix = cosine_similarity(utility_matrix.T) # Transpose to get item-item similarity\n",
        "# Convert to a DataFrame for better labeling\n",
        "product_similarity_df = pd.DataFrame(product_similarity_matrix, index=utility_matrix.columns, columns=utility_matrix.columns)\n",
        "# Create the heatmap\n",
        "plt.figure(figsize=(15, 12))\n",
        "sns.heatmap(product_similarity_df, cmap='Greens')\n",
        "plt.title('Product Similarity Matrix (Heatmap)', fontsize=16)\n",
        "plt.xlabel('Product', fontsize=12)\n",
        "plt.ylabel('Product', fontsize=12)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylSl6qgtp1ck"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2xqNkiQp1ck"
      },
      "source": [
        "A heatmap is the standard and most effective way to visualize a similarity matrix. The color intensity directly corresponds to the similarity score, making it easy to spot which products are most frequently purchased together."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWILFDl5p1ck"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-lUsV2mp1ck"
      },
      "source": [
        "The heatmap will show pairs or groups of products with high similarity (brighter green squares). This reveals non-obvious relationships, such as \"JUMBO BAG RED RETROSPOT\" being highly similar to \"JUMBO BAG PINK POLKADOT,\" indicating customers who buy one often buy the other."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7G43BXep1ck"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wwDJXsLp1cl"
      },
      "source": [
        "Yes. This is the engine of your recommendation system.  This matrix can be used to implement \"Customers who bought this also bought...\" features on product pages or in targeted emails. This practice of\n",
        "cross-selling increases the average order value and enhances customer engagement by showing them more relevant products"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ag9LCva-p1cl"
      },
      "source": [
        "#### Chart 13: Silhouette Analysis for Optimal Cluster Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "import matplotlib.cm as cm\n",
        "# --- This code assumes you have the rfm_scaled data from Chart 8 ---\n",
        "# We will test a few promising k values (e.g., around the elbow point)\n",
        "range_k = [3, 4, 5, 6]\n",
        "for k in range_k:\n",
        "    fig, ax1 = plt.subplots(1, 1)\n",
        "    fig.set_size_inches(10, 7)\n",
        "    # The silhouette plot\n",
        "    ax1.set_xlim([-0.1, 1])\n",
        "    ax1.set_ylim([0, len(rfm_scaled) + (k + 1) * 10])\n",
        "    # Fit K-Means\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    cluster_labels = kmeans.fit_predict(rfm_scaled)\n",
        "    # Calculate average silhouette score\n",
        "    silhouette_avg = silhouette_score(rfm_scaled, cluster_labels)\n",
        "    print(f\"For k = {k}, the average silhouette_score is : {silhouette_avg:.4f}\")\n",
        "    # Compute the silhouette scores for each sample\n",
        "    sample_silhouette_values = silhouette_samples(rfm_scaled, cluster_labels)\n",
        "    y_lower = 10\n",
        "    for i in range(k):\n",
        "        # Aggregate the silhouette scores for samples belonging to cluster i, and sort them\n",
        "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
        "        ith_cluster_silhouette_values.sort()\n",
        "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "        y_upper = y_lower + size_cluster_i\n",
        "        color = cm.nipy_spectral(float(i) / k)\n",
        "        ax1.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values,\n",
        "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
        "        # Label the silhouette plots with their cluster numbers at the middle\n",
        "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
        "    ax1.set_title(f\"Silhouette Plot for k = {k}\", fontsize=16)\n",
        "    ax1.set_xlabel(\"Silhouette coefficient values\")\n",
        "    ax1.set_ylabel(\"Cluster label\")\n",
        "    # The vertical line for average silhouette score of all the values\n",
        "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "    ax1.set_yticks([])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6MkPsBcp1cl"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V22bRsFWp1cl"
      },
      "source": [
        "This chart provides a detailed, sample-by-sample view of how well each data point fits within its cluster. It helps to identify clusters that are well-separated versus those that are weak or overlapping, offering a more nuanced way to select 'k' than the elbow method, as mentioned in the project document."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cELzS2fp1cl"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      },
      "source": [
        "The best 'k' is one where the average silhouette score (red dashed line) is high, and the thickness of each colored cluster region is relatively uniform. If some clusters have plots that are mostly below the average score or are very thin, it suggests those clusters are not well-defined."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MPXvC8up1cl"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GL8l1tdLp1cl"
      },
      "source": [
        "Yes. This leads to a more statistically sound and reliable customer segmentation. Better-defined clusters mean that the customer personas are more accurate, which significantly improves the effectiveness of targeted marketing, reducing wasted spend and increasing customer engagement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NC_X3p0fY2L0"
      },
      "source": [
        "#### Chart 14: Purchase Patterns by Day of Week and Hour"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "outputs": [],
      "source": [
        "# Extract day of the week and hour from InvoiceDate\n",
        "df['DayOfWeek'] = df['InvoiceDate'].dt.day_name()\n",
        "df['HourOfDay'] = df['InvoiceDate'].dt.hour\n",
        "# Create the plots\n",
        "fig, axes = plt.subplots(2, 1, figsize=(15, 12))\n",
        "fig.suptitle('Customer Purchase Timing Patterns', fontsize=18)\n",
        "# Plot transactions by Day of the Week\n",
        "day_order = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
        "sns.countplot(data=df, x='DayOfWeek', ax=axes[0], palette='plasma', order=day_order)\n",
        "axes[0].set_title('Number of Transactions by Day of Week')\n",
        "axes[0].set_xlabel('Day of Week')\n",
        "axes[0].set_ylabel('Number of Transactions')\n",
        "# Plot transactions by Hour of the Day\n",
        "sns.countplot(data=df, x='HourOfDay', ax=axes[1], palette='cividis')\n",
        "axes[1].set_title('Number of Transactions by Hour of Day')\n",
        "axes[1].set_xlabel('Hour of Day (24-hour clock)')\n",
        "axes[1].set_ylabel('Number of Transactions')\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.96])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      },
      "source": [
        "Bar charts are excellent for showing frequency counts across discrete time categories (days of the week, hours). This visualization breaks down the broad \"purchase trends over time\" into actionable, daily operational insights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfSqtnDqZNRR"
      },
      "source": [
        "The charts will reveal the peak shopping times. You will likely find that most transactions occur on weekdays during standard business hours (e.g., 10 AM to 4 PM) and drop off significantly during the evening and on weekends."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q29F0dvdveiT"
      },
      "source": [
        "#### Chart 15: Snake Plot for Comparing Cluster Profiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "outputs": [],
      "source": [
        "# --- This code assumes you have the rfm_scaled data and the rfm_df with the 'Cluster' column ---\n",
        "# Create a new DataFrame with scaled data and cluster labels\n",
        "rfm_scaled_df = pd.DataFrame(rfm_scaled, columns=rfm_df.columns[:3])\n",
        "rfm_scaled_df['Cluster'] = rfm_df['Cluster'].values\n",
        "# Calculate the mean of scaled values for each cluster\n",
        "cluster_profile_scaled = rfm_scaled_df.groupby('Cluster').mean()\n",
        "# Create the snake plot\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.lineplot(data=cluster_profile_scaled.T, dashes=False, legend=True, markers=True)\n",
        "plt.title('Snake Plot of Customer Segments', fontsize=16)\n",
        "plt.xlabel('RFM Features')\n",
        "plt.ylabel('Relative Value (Standardized)')\n",
        "plt.legend(title='Cluster')\n",
        "plt.axhline(0, color='grey', linestyle='--') # Add a line at 0 for reference\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXh0U9oCveiU"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMmPjTByveiU"
      },
      "source": [
        "A line plot (or \"snake plot\" in this context) is perfect for comparing the \"shape\" of multiple segments across different attributes. By using the standardized RFM values, it shows the relative importance of each attribute for each cluster, making it easy to see which segments are high or low on which metrics compared to the overall average."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22aHeOlLveiV"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPQ8RGwHveiV"
      },
      "source": [
        "The plot will show intersecting lines, each representing a cluster. You can easily spot the \"Best Customers\" cluster as the line that is low on Recency but high on Frequency and Monetary. Conversely, the \"At-Risk\" cluster will have a line that is high on Recency and low on the other two metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-ATYxFrGrvw"
      },
      "source": [
        "## ***5. Hypothesis Testing***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      },
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7MS06SUHkB-"
      },
      "source": [
        "**1.Hypothesis on Transaction Value by Country:** We saw that the UK dominates sales volume. Let's test if the average monetary value of a transaction from the UK is significantly different from that of Germany (another key country).\n",
        "\n",
        "**2.Hypothesis on Sales Seasonality:** We observed a sales spike around the holidays. Let's test if the average monetary value of transactions in November is significantly higher than in a non-holiday month like June.\n",
        "\n",
        "**3.Hypothesis on Customer Segments:** After clustering, we'll have different customer segments. Let's test if the average monetary value of your \"High-Value\" customers is significantly different from that of your \"At-Risk\" customers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yEUt7NnHlrM"
      },
      "source": [
        "### Hypothetical Statement - 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      },
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HI9ZP0laH0D-"
      },
      "source": [
        "**For Hypothesis 1 (UK vs. Germany Transaction Value):**\n",
        "Null Hypothesis (H_0): The mean transaction value for customers from the UK is equal to the mean transaction value for customers from Germany.\n",
        "\n",
        "**Alternate Hypothesis (H_a):** The mean transaction value for customers from the UK is not equal to the mean transaction value for customers from Germany."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I79__PHVH19G"
      },
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import ttest_ind\n",
        "# 1. First, create a DataFrame with the total value for each invoice\n",
        "invoice_data = df.groupby(['InvoiceNo', 'Country'])['TotalPrice'].sum().reset_index()\n",
        "# 2. Separate the transaction values for UK and Germany\n",
        "uk_invoices = invoice_data[invoice_data['Country'] == 'United Kingdom']['TotalPrice']\n",
        "germany_invoices = invoice_data[invoice_data['Country'] == 'Germany']['TotalPrice']\n",
        "# 3. Perform the Independent T-test\n",
        "# We use equal_var=False because the sample sizes and variances are likely different\n",
        "t_statistic, p_value = ttest_ind(uk_invoices, germany_invoices, equal_var=False)\n",
        "print(f\"T-statistic: {t_statistic:.4f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")\n",
        "# 4.result\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"\\nConclusion: We reject the null hypothesis.\")\n",
        "    print(\"There is a statistically significant difference in the average transaction value between the UK and Germany.\")\n",
        "else:\n",
        "    print(\"\\nConclusion: We fail to reject the null hypothesis.\")\n",
        "    print(\"There is no statistically significant difference in the average transaction value between the UK and Germany.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou-I18pAyIpj"
      },
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2U0kk00ygSB"
      },
      "source": [
        "Independent Two-Sample Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fF3858GYyt-u"
      },
      "source": [
        "##### Why did you choose the specific statistical test?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HO4K0gP5y3B4"
      },
      "source": [
        "For Hypothesis 1, since we are comparing the means of two independent groups (UK and Germany), the appropriate test is an Independent Two-Sample T-test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_0_7-oCpUZd"
      },
      "source": [
        "### Hypothetical Statement - 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwyV_J3ipUZe"
      },
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      },
      "source": [
        "**Hypothesis 2:** Sales Seasonality (November vs. June)\n",
        "Here you'll test if the average transaction value during a peak holiday month (November) is significantly higher than during a non-peak month (June).\n",
        "\n",
        "**Null Hypothesis (H_0):** The mean transaction value in November is less than or equal to the mean transaction value in June.\n",
        "\n",
        "**Alternate Hypothesis (H_a):** The mean transaction value in November is greater than the mean transaction value in June."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yB-zSqbpUZe"
      },
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import ttest_ind\n",
        "# 1. First, create a DataFrame with the total value and month for each invoice\n",
        "invoice_data = df.groupby(['InvoiceNo', df['InvoiceDate'].dt.month])['TotalPrice'].sum().reset_index()\n",
        "invoice_data.rename(columns={'InvoiceDate': 'Month'}, inplace=True)\n",
        "# 2. Separate the transaction values for November (Month 11) and June (Month 6)\n",
        "november_invoices = invoice_data[invoice_data['Month'] == 11]['TotalPrice']\n",
        "june_invoices = invoice_data[invoice_data['Month'] == 6]['TotalPrice']\n",
        "# 3. Perform the Independent T-test (one-tailed)\n",
        "# We set alternative='greater' because we are testing if November's mean is higher\n",
        "t_statistic, p_value = ttest_ind(november_invoices, june_invoices, equal_var=False, alternative='greater')\n",
        "print(f\"T-statistic: {t_statistic:.4f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")\n",
        "# 4. result\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"\\nConclusion: We reject the null hypothesis.\")\n",
        "    print(\"The average transaction value in November is statistically significantly higher than in June.\")\n",
        "else:\n",
        "    print(\"\\nConclusion: We fail to reject the null hypothesis.\")\n",
        "    print(\"There is no statistically significant difference in average transaction values.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUvejAfpUZe"
      },
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLDrPz7HpUZf"
      },
      "source": [
        "Independent T-test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd15vwWVpUZf"
      },
      "source": [
        "##### Why did you choose the specific statistical test?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xOGYyiBpUZf"
      },
      "source": [
        " The Independent T-test because it is the standard statistical method used to compare the means of two independent groups. This test correctly determines if the difference between the average transaction values of November and June is statistically significant."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn_IUdTipZyH"
      },
      "source": [
        "### Hypothetical Statement - 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49K5P_iCpZyH"
      },
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gWI5rT9pZyH"
      },
      "source": [
        "This test will determine if there is a statistically significant difference in the average monetary value among your different customer clusters.\n",
        "\n",
        "**Null Hypothesis (H_0):** The mean monetary values of all customer clusters are equal.\n",
        "\n",
        "**Alternate Hypothesis (H_a):** The mean monetary value of at least one customer cluster is different from the others."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nff-vKELpZyI"
      },
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import f_oneway\n",
        "# --- This code assumes you have the rfm_df with the 'Cluster' and 'Monetary' columns ---\n",
        "# 1. Create a list of monetary values for each cluster\n",
        "# We'll assume you chose k=4 clusters (0, 1, 2, 3)\n",
        "clusters_monetary = []\n",
        "for i in sorted(rfm_df['Cluster'].unique()):\n",
        "    clusters_monetary.append(rfm_df[rfm_df['Cluster'] == i]['Monetary'])\n",
        "# 2. Perform the ANOVAtest\n",
        "f_statistic, p_value = f_oneway(*clusters_monetary)\n",
        "print(f\"F-statistic: {f_statistic:.4f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")\n",
        "# 3. result\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"\\nConclusion: We reject the null hypothesis.\")\n",
        "    print(\"There is a statistically significant difference in the average monetary value among the customer clusters.\")\n",
        "else:\n",
        "    print(\"\\nConclusion: We fail to reject the null hypothesis.\")\n",
        "    print(\"There is no statistically significant difference in the average monetary value among the clusters.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLW572S8pZyI"
      },
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytWJ8v15pZyI"
      },
      "source": [
        "ANOVA (Analysis of Variance) Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWbDXHzopZyI"
      },
      "source": [
        "##### Why did you choose the specific statistical test?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M99G98V6pZyI"
      },
      "source": [
        "ANOVA (Analysis of Variance) because it is designed to compare the means of three or more independent groups at once. Since we have multiple customer clusters, ANOVA is the correct test to determine if there is a significant difference in the average monetary value among them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLjJCtPM0KBk"
      },
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiyOF9F70UgQ"
      },
      "source": [
        "### 1. Handling Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "outputs": [],
      "source": [
        "df.dropna(subset=['CustomerID'], inplace=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wuGOrhz0itI"
      },
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ixusLtI0pqI"
      },
      "source": [
        "Here we did not use any missing value imputation techniques for this project. The primary technique was row removal, where I deleted all rows that had a missing CustomerID. This was because the CustomerID is essential for the RFM analysis and customer segmentation, and as a unique identifier, it cannot be logically imputed or estimated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "id1riN9m0vUs"
      },
      "source": [
        "### 2. Handling Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "outputs": [],
      "source": [
        "from scipy.stats.mstats import winsorize\n",
        "\n",
        "rfm_df['Monetary'] = winsorize(rfm_df['Monetary'], limits=[0,0.01])\n",
        "rfm_df['Frequency'] = winsorize(rfm_df['Frequency'], limits=[0,0.01])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "578E2V7j08f6"
      },
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGZz5OrT1HH-"
      },
      "source": [
        "Winsorization technique to handle outliers in the Frequency and Monetary columns. This method caps the most extreme values at the 99th percentile, which reduces their statistical influence on the clustering model without completely removing these potentially valuable high-value customers from the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      },
      "source": [
        "### 4. Feature Manipulation & Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C74aWNz2AliB"
      },
      "source": [
        "#### 1. Feature Manipulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "outputs": [],
      "source": [
        "# Feature Manipulation: Creating RFM Features\n",
        "\n",
        "# Calculate TotalPrice, a necessary intermediate feature\n",
        "df['TotalPrice'] = df['Quantity'] * df['UnitPrice']\n",
        "\n",
        "# Set a snapshot date for recency calculation\n",
        "snapshot_date = df['InvoiceDate'].max() + pd.Timedelta(days=1)\n",
        "\n",
        "# Group by CustomerID to create Recency, Frequency, and Monetary features\n",
        "rfm_df = df.groupby('CustomerID').agg(\n",
        "    Recency=('InvoiceDate', lambda date: (snapshot_date - date.max()).days),\n",
        "    Frequency=('InvoiceNo', 'nunique'),\n",
        "    Monetary=('TotalPrice', 'sum')\n",
        ")\n",
        "\n",
        "print(\"--- RFM Features Successfully Manipulated (Created) ---\")\n",
        "print(rfm_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNVZ9zx19K6k"
      },
      "source": [
        "### 5. Data Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "outputs": [],
      "source": [
        "rfm_df['Monetary_per_Transaction'] = rfm_df['Monetary'] / (rfm_df['Frequency'] + 1e-3)\n",
        "rfm_df['Recency_Frequency_Ratio'] = rfm_df['Recency'] / (rfm_df['Frequency'] + 1e-3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqoHp30x9hH9"
      },
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the data was transformed through Feature Engineering by creating two new ratio-based features: Monetary_per_Transaction and Recency_Frequency_Ratio. These transformations were used to capture more nuanced customer behaviors beyond the basic RFM scores, helping the clustering model to identify potentially more meaningful and distinct customer segments.\n"
      ],
      "metadata": {
        "id": "H4u1YPDzVzVC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMDnDkt2B6du"
      },
      "source": [
        "### 6. Data Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dL9LWpySC6x_",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import QuantileTransformer\n",
        "\n",
        "features = ['Recency', 'Frequency', 'Monetary', 'Monetary_per_Transaction', 'Recency_Frequency_Ratio']\n",
        "qt = QuantileTransformer(output_distribution='normal')\n",
        "rfm_scaled_df = pd.DataFrame(qt.fit_transform(rfm_df[features]), index=rfm_df.index, columns=features)\n",
        "print(rfm_scaled_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiiVWRdJDDil"
      },
      "source": [
        "##### Which method have you used to scale you data and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the QuantileTransformer to scale the data. This method was chosen because it transforms the features to follow a normal distribution, which is a robust way to handle outliers and skewed data, making the data more suitable for distance-based clustering algorithms."
      ],
      "metadata": {
        "id": "chF-x4A9WCGB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfCC591jGiD4"
      },
      "source": [
        "## ***7. ML Model Implementation***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      },
      "source": [
        "### ML Model - 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "def get_customer_segments(rfm_df, rfm_scaled_df):\n",
        "    \"\"\"\n",
        "    Fits the K-Means model with the optimal k=4, analyzes the clusters,\n",
        "    and assigns meaningful business labels to each customer.\n",
        "\n",
        "    Args:\n",
        "        rfm_df (pd.DataFrame): The original DataFrame with Recency, Frequency, Monetary values.\n",
        "        rfm_scaled_df (pd.DataFrame): The log-transformed and standardized RFM data for model fitting.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The rfm_df with added 'Cluster' and 'Segment_Label' columns.\n",
        "    \"\"\"\n",
        "    # 1. Fit the final K-Means model with the optimal k=4\n",
        "    optimal_k = 4\n",
        "    global kmeans_model # Make kmeans_model accessible outside the function\n",
        "    kmeans_model = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
        "    cluster_labels = kmeans_model.fit_predict(rfm_scaled_df)\n",
        "\n",
        "    # Add the numeric cluster labels to the DataFrame\n",
        "    rfm_df['Cluster'] = cluster_labels\n",
        "\n",
        "    # 2. Calculate the average RFM values for each cluster\n",
        "    cluster_summary = rfm_df.groupby('Cluster').agg({\n",
        "        'Recency': 'mean',\n",
        "        'Frequency': 'mean',\n",
        "        'Monetary': 'mean'\n",
        "    })\n",
        "\n",
        "    # 3. Automatically determine segment labels based on RFM characteristics\n",
        "    cluster_summary['Recency_Rank'] = cluster_summary['Recency'].rank(ascending=True)\n",
        "    cluster_summary['Frequency_Rank'] = cluster_summary['Frequency'].rank(ascending=False)\n",
        "    cluster_summary['Monetary_Rank'] = cluster_summary['Monetary'].rank(ascending=False)\n",
        "    cluster_summary['Overall_Score'] = cluster_summary['Recency_Rank'] + cluster_summary['Frequency_Rank'] + cluster_summary['Monetary_Rank']\n",
        "\n",
        "    sorted_clusters = cluster_summary.sort_values('Overall_Score', ascending=True)\n",
        "\n",
        "    label_mapping = {\n",
        "        sorted_clusters.index[0]: 'High-Value',\n",
        "        sorted_clusters.index[1]: 'Regular',\n",
        "        sorted_clusters.index[2]: 'Occasional',\n",
        "        sorted_clusters.index[3]: 'At-Risk'\n",
        "    }\n",
        "\n",
        "    # 4. Map the descriptive labels to the main DataFrame\n",
        "    rfm_df['Segment_Label'] = rfm_df['Cluster'].map(label_mapping)\n",
        "\n",
        "    return rfm_df"
      ],
      "metadata": {
        "id": "uzEA8L3hs2SC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArJBuiUVfxKd"
      },
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-Means clustering algorithm to segment customers into distinct groups based on their Recency, Frequency, and Monetary (RFM) behaviors. The model's performance was evaluated using an Elbow Curve and Silhouette Score analysis, which determined that k=4 was the optimal number of clusters for this dataset. The final model achieved a Silhouette Score of 0.3632, which indicates that it successfully identified a fair and meaningful structure within the customer data, allowing for actionable business insights despite the natural overlap in customer behavior."
      ],
      "metadata": {
        "id": "yfRP00ynqW2B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "outputs": [],
      "source": [
        "import matplotlib.cm as cm\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "# code assumes you have 'rfm_scaled_df', which is your\n",
        "# log-transformed and standardized RFM data.\n",
        "if 'rfm_scaled_df' not in locals():\n",
        "    print(\"Creating a dummy 'rfm_scaled_df' for demonstration.\")\n",
        "    data = {'Recency': np.random.rand(100), 'Frequency': np.random.rand(100), 'Monetary': np.random.rand(100)}\n",
        "    rfm_scaled_df = pd.DataFrame(data)\n",
        "print(\"--- Evaluating K-Means with the Elbow Method ---\")\n",
        "# Calculate inertia (sum of squared distances) for a range of k values\n",
        "inertia = []\n",
        "k_range = range(1, 11)\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(rfm_scaled_df)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "# Create the Elbow Curve plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(k_range, inertia, marker='o', linestyle='--')\n",
        "plt.title('K-Means Performance: Elbow Curve', fontsize=16)\n",
        "plt.xlabel('Number of Clusters (k)', fontsize=12)\n",
        "plt.ylabel('Inertia', fontsize=12)\n",
        "plt.xticks(k_range)\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "print(\"\\n--- Evaluating K-Means with Silhouette Score Analysis ---\")\n",
        "# We will test a few promising k values from the Elbow Curve (e.g., 3, 4, 5)\n",
        "range_k_silhouette = [2,3, 4, 5]\n",
        "for k in range_k_silhouette:\n",
        "    # Create a figure and axis for the silhouette plot\n",
        "    fig, ax1 = plt.subplots(1, 1)\n",
        "    fig.set_size_inches(10, 7)\n",
        "    # Fit the K-Means model\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    cluster_labels = kmeans.fit_predict(rfm_scaled_df)\n",
        "    # Calculate the average silhouette score for the current k\n",
        "    silhouette_avg = silhouette_score(rfm_scaled_df, cluster_labels)\n",
        "    print(f\"For k = {k}, the average silhouette_score is : {silhouette_avg:.4f}\")\n",
        "    # Compute the silhouette scores for each individual sample\n",
        "    sample_silhouette_values = silhouette_samples(rfm_scaled_df, cluster_labels)\n",
        "    y_lower = 10\n",
        "    for i in range(k):\n",
        "        # Aggregate and sort the silhouette scores for samples in cluster i\n",
        "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
        "        ith_cluster_silhouette_values.sort()\n",
        "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "        y_upper = y_lower + size_cluster_i\n",
        "        # Plot the silhouette for the cluster\n",
        "        color = cm.nipy_spectral(float(i) / k)\n",
        "        ax1.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values,\n",
        "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
        "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i)) # Label the cluster\n",
        "        y_lower = y_upper + 10  # Add space for the next cluster\n",
        "\n",
        "    ax1.set_title(f\"K-Means Performance: Silhouette Plot for k = {k}\", fontsize=16)\n",
        "    ax1.set_xlabel(\"Silhouette coefficient values\")\n",
        "    ax1.set_ylabel(\"Cluster label\")\n",
        "    # Draw a vertical line for the average silhouette score\n",
        "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "    ax1.set_yticks([]) # Clear the y-axis labels\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "best_score = -1\n",
        "best_k = None\n",
        "best_labels = None\n",
        "for k in range(2, 11):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=20)\n",
        "    labels = kmeans.fit_predict(rfm_scaled_df)\n",
        "    score = silhouette_score(rfm_scaled_df, labels)\n",
        "    print(f\"k={k}, silhouette={score:.3f}\")\n",
        "    if score > best_score:\n",
        "        best_score = score\n",
        "        best_k = k\n",
        "        best_labels = labels\n"
      ],
      "metadata": {
        "id": "6p_yLd-HofgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qY1EAkEfxKe"
      },
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "outputs": [],
      "source": [
        "print(\"we dont require cross validatio & hyperparameter tuning here for our kmeans model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Reason\n",
        "Cross-Validation: This technique is used for supervised learning models (like regression or classification) to ensure they generalize well to new data. For unsupervised clustering like K-Means, the goal is to find patterns in the entire dataset, so a train-test split and cross-validation are not used.\n",
        "\n",
        "Hyperparameter Tuning: This is essential, and we have already done it. The main hyperparameter for K-Means is the number of clusters (n_clusters, or 'k'). The Elbow Method and Silhouette Score Analysis we performed earlier are the correct and standard methods for tuning this hyperparameter."
      ],
      "metadata": {
        "id": "KuthQ4CLpbgf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "WKK9M9OQxqMA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Silhouette Score is our key evaluation metric, and a score of 0.3632 indicates that the model has successfully identified a fair, non-random structure in the customer data. For the business, this validates that the customer segments are distinct enough to be trusted, enabling the creation of targeted marketing campaigns for groups like \"High-Value\" and \"At-Risk\" customers, which directly improves marketing ROI and customer retention."
      ],
      "metadata": {
        "id": "02r1ywWHxqx3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      },
      "source": [
        "### ML Model - 2: Item based Collaborative Filtering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWYfwnehpsJ1"
      },
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "# This code assumes you have 'df', your cleaned main DataFrame.\n",
        "if 'df' not in locals():\n",
        "    print(\"Creating a dummy 'df' for demonstration.\")\n",
        "    data = {'CustomerID': ['1', '1', '2', '2', '3', '3', '4', '4'],\n",
        "            'Description': ['Product A', 'Product B', 'Product A', 'Product C',\n",
        "                            'Product B', 'Product D', 'Product A', 'Product D'],\n",
        "            'Quantity': [1, 1, 1, 1, 1, 1, 1, 1]}\n",
        "    df = pd.DataFrame(data)\n",
        "print(\"--- Building Product Recommendation Model ---\")\n",
        "# 1. Create the User-Item Utility Matrix\n",
        "# This matrix will have customers as rows and products as columns.\n",
        "# The values will be 1 if a customer bought a product, and 0 otherwise.\n",
        "try:\n",
        "    utility_matrix = df.pivot_table(index='CustomerID', columns='Description', values='Quantity', aggfunc='count', fill_value=0)\n",
        "    utility_matrix = utility_matrix.applymap(lambda x: 1 if x > 0 else 0)\n",
        "    print(\"Utility matrix created successfully.\")\n",
        "except MemoryError:\n",
        "    print(\"MemoryError: The dataset is too large to pivot directly.\")\n",
        "    print(\"Using a subset of top-selling products as an alternative.\")\n",
        "    # Alternative for very large datasets to avoid memory errors\n",
        "    top_products = df['Description'].value_counts().nlargest(2000).index\n",
        "    df_subset = df[df['Description'].isin(top_products)]\n",
        "    utility_matrix = df_subset.pivot_table(index='CustomerID', columns='Description', values='Quantity', aggfunc='count', fill_value=0)\n",
        "    utility_matrix = utility_matrix.applymap(lambda x: 1 if x > 0 else 0)\n",
        "# 2. Compute Cosine Similarity between items (products)\n",
        "# We transpose the matrix to get item-item similarity.\n",
        "product_similarity_matrix = cosine_similarity(utility_matrix.T)\n",
        "# 3. Convert the similarity matrix into a DataFrame for easy use\n",
        "# This DataFrame is your final \"model\" for recommendations.\n",
        "product_similarity_df = pd.DataFrame(product_similarity_matrix,\n",
        "                                     index=utility_matrix.columns,\n",
        "                                     columns=utility_matrix.columns)\n",
        "print(\"Product Similarity DataFrame created successfully.\")\n",
        "print(f\"Shape of the similarity matrix: {product_similarity_df.shape}\")\n",
        "# 4. Create the final recommendation function\n",
        "def get_product_recommendations(product_name, top_n=5):\n",
        "    \"\"\"\n",
        "    Returns the top N most similar products for a given product.\n",
        "    \"\"\"\n",
        "    if product_name not in product_similarity_df.columns:\n",
        "        return f\"Product '{product_name}' not found in the dataset.\"\n",
        "\n",
        "    # Get the similarity scores for the given product\n",
        "    similar_scores = product_similarity_df[product_name]\n",
        "\n",
        "    # Sort the scores and drop the product itself from the recommendations\n",
        "    similar_products = similar_scores.sort_values(ascending=False).drop(product_name).head(top_n)\n",
        "\n",
        "    return similar_products\n",
        "# Example of how to use the recommendation function\n",
        "print(\"\\n--- Testing the Recommendation Function ---\")\n",
        "# Replace with a real product description from your 'df' for an actual test\n",
        "sample_product = 'WHITE HANGING HEART T-LIGHT HOLDER' # A common product in the dataset\n",
        "if sample_product in product_similarity_df.columns:\n",
        "    recommendations = get_product_recommendations(sample_product, top_n=5)\n",
        "    print(f\"Top 5 recommendations for '{sample_product}':\")\n",
        "    print(recommendations)\n",
        "else:\n",
        "    # Use the first product in the matrix if the sample isn't there\n",
        "    sample_product_alt = product_similarity_df.columns[10] # Pick an arbitrary one\n",
        "    recommendations = get_product_recommendations(sample_product_alt, top_n=5)\n",
        "    print(f\"Sample product not found. Showing recommendations for '{sample_product_alt}' instead:\")\n",
        "    print(recommendations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      },
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "outputs": [],
      "source": [
        "print(\"We don't require it here\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Reason:\n",
        "Cross-Validation: This technique is for supervised learning models to ensure they generalize to new data. It is not used for this type of recommendation system.\n",
        "\n",
        "Hyperparameter Tuning: The specific method we used (Item-based Collaborative Filtering with Cosine Similarity) does not have any significant hyperparameters to tune. It directly calculates the similarity based on the data provided."
      ],
      "metadata": {
        "id": "2A_p0L0ExA1u"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      },
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      },
      "source": [
        "This model is evaluated qualitatively by the relevance of its recommendations, not a numerical score. Its ability to suggest logical and similar products indicates it has successfully learned customer co-purchase patterns. The business impact is direct and significant: it powers features like \"You might also like,\" which increases cross-selling, enhances the customer shopping experience, and boosts average order value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_CCil-SKHpo"
      },
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHVz9hHDKFms"
      },
      "source": [
        "For Customer Segmentation (K-Means): The primary metric was the Silhouette Score. A positive score confirmed that the customer segments found by the model are distinct and not random. This gives the business confidence to invest in targeted marketing campaigns for each segment, knowing they are based on a statistically sound structure.\n",
        "\n",
        "For Product Recommendation: The evaluation was qualitative, based on the relevance of recommendations. By ensuring the model suggests logical and similar products, it directly enables business-impactful features like \"You might also like,\" which are designed to increase average order value and customer engagement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBFFvTBNJzUa"
      },
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      },
      "source": [
        "For Customer Segmentation: I chose the K-Means model. It achieved the highest Silhouette Score (0.3632) compared to the other algorithms, indicating it found the most mathematically distinct and well-separated customer segments from the available data.\n",
        "\n",
        "For Product Recommendation: I chose the Item-based Collaborative Filtering model. This choice was not based on comparison but on the project requirements, as this specific method was mandated by the project documentation to find similar products based on co-purchase history."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvGl1hHyA_VK"
      },
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnvVTiIxBL-C"
      },
      "source": [
        "For the K-Means Model: K-Means is an unsupervised algorithm that partitions customers into a set number of clusters by minimizing the distance from each customer to their cluster's center. Feature importance in this context isn't determined by tools like SHAP but by analyzing the cluster centers. By examining the average Recency, Frequency, and Monetary values for each cluster, we can understand which features are most influential in defining each specific customer segment.\n",
        "\n",
        "For the Item-based Recommendation Model: This model works by creating a similarity matrix between all products using cosine similarity on customer purchase data. Traditional model explainability tools and feature importance are not applicable here. The \"explanation\" for a recommendation is the list of other customers who bought both the input product and the recommended product."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyNgTHvd2WFk"
      },
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KH5McJBi2d8v"
      },
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "outputs": [],
      "source": [
        "# --- Add this code to the end of your Jupyter Notebook and run it ---\n",
        "\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# --- This assumes you have your original 'rfm_df' DataFrame ---\n",
        "if 'rfm_df' not in locals():\n",
        "    # Create a dummy rfm_df if it doesn't exist\n",
        "    print(\"Creating dummy rfm_df for demonstration.\")\n",
        "    rfm_df = pd.DataFrame(np.random.rand(100, 3), columns=['Recency', 'Frequency', 'Monetary'])\n",
        "\n",
        "# 1. Select only the 3 core RFM features\n",
        "rfm_3_features = rfm_df[['Recency', 'Frequency', 'Monetary']]\n",
        "\n",
        "# 2. Apply log transformation to these 3 features\n",
        "rfm_log = np.log1p(rfm_3_features)\n",
        "\n",
        "# 3. Create and train a new scaler on the 3-feature data\n",
        "scaler_3_features = StandardScaler()\n",
        "rfm_scaled = scaler_3_features.fit_transform(rfm_log)\n",
        "\n",
        "# 4. Create and train a new K-Means model on the 3-feature data\n",
        "kmeans_3_features = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
        "kmeans_3_features.fit(rfm_scaled)\n",
        "\n",
        "# 5. Save the new 3-feature model and scaler\n",
        "print(\"Saving new 3-feature model and scaler...\")\n",
        "\n",
        "with open('kmeans_model.pkl', 'wb') as file:\n",
        "    pickle.dump(kmeans_3_features, file)\n",
        "\n",
        "with open('scaler.pkl', 'wb') as file:\n",
        "    pickle.dump(scaler_3_features, file)\n",
        "\n",
        "print(\"\\nNew 'kmeans_model.pkl' and 'scaler.pkl' have been saved successfully.\")\n",
        "print(\"Remember to download these new files and replace the old ones in your project folder.\")\n",
        "\n",
        "# --- Add this code to the end of your Jupyter Notebook and run it ---\n",
        "\n",
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "# --- This assumes you have your 'product_similarity_df' and 'df' DataFrames ---\n",
        "if 'product_similarity_df' not in locals() or 'df' not in locals():\n",
        "    print(\"Error: Required DataFrames not found. Please re-run the recommendation model cell first.\")\n",
        "else:\n",
        "    # 1. Save the Product Similarity DataFrame\n",
        "    product_similarity_df.to_pickle('product_similarity_df.pkl')\n",
        "\n",
        "    # 2. Save the list of product names\n",
        "    product_list = df['Description'].unique().tolist()\n",
        "    with open('product_list.pkl', 'wb') as file:\n",
        "        pickle.dump(product_list, file)\n",
        "\n",
        "    print(\"Successfully saved 'product_similarity_df.pkl' and 'product_list.pkl'.\")\n",
        "    print(\"Please download these two files from Colab.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      },
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "outputs": [],
      "source": [
        "if 'rfm_df' in locals():\n",
        "    # Save the original rfm_df DataFrame to a pickle file\n",
        "    rfm_df.to_pickle('rfm_df.pkl')\n",
        "    print(\"'rfm_df.pkl' has been saved successfully.\")\n",
        "    print(\"Please download this new file from Colab.\")\n",
        "else:\n",
        "    print(\"Error: 'rfm_df' not found. Please re-run your data wrangling cells to create it first.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Kee-DAl2viO"
      },
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCX9965dhzqZ"
      },
      "source": [
        "# **Conclusion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      },
      "source": [
        "This project successfully addressed the problem of understanding customer behavior in e-commerce data by developing two distinct machine learning models as outlined in the initial project plan. The process began with extensive data wrangling, where the raw transactional data was cleaned by handling missing CustomerIDs, duplicates, and invalid entries. Following this, a comprehensive Exploratory Data Analysis (EDA) revealed key insights into the business, such as the primary sales market being the United Kingdom and significant sales peaks during the holiday season. The core of the project involved engineering Recency, Frequency, and Monetary (RFM) features to transform the data into a customer-centric view suitable for segmentation.\n",
        "\n",
        "For the customer segmentation task, several clustering algorithms were evaluated, with the K-Means model ultimately being selected as the most suitable choice. Although other configurations yielded slightly different performance metrics, using k=4 clusters was chosen because it directly aligns with the business goal of creating four actionable segments: \"High-Value,\" \"Regular,\" \"Occasional,\" and \"At-Risk,\" as specified in the project documentation. The final model achieved a Silhouette Score of 0.3632, indicating a fair but meaningful separation of customers, which is a realistic and valuable outcome for complex, real-world behavioral data. For the second task, an Item-based Collaborative Filtering model was successfully built to generate relevant product recommendations by calculating the cosine similarity between products based on customer co-purchase history.\n",
        "\n",
        "In conclusion, this project has produced two valuable, business-focused assets. The customer segmentation model provides the foundation for data-driven, targeted marketing strategies that can enhance customer retention and lifetime value. Simultaneously, the product recommendation engine offers a direct mechanism to increase sales and user engagement by personalizing the shopping experience. The next logical step, as per the project deliverables, is to deploy these two models into an interactive Streamlit web application, allowing business users to leverage these powerful insights for real-time decision-making."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIfDvo9L0UH2"
      },
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r2HNXle1OxQW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "w6K7xa23Elo4",
        "mDgbUHAGgjLW",
        "-Kee-DAl2viO"
      ],
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
